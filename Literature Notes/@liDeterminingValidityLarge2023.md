---
kind: Literature Note
tags: Artificial Intelligence, Algorithmic Marketing, Language Models, Market Research
citekey: liDeterminingValidityLarge2023
status: unread
url: https://papers.ssrn.com/abstract=4241291
dateread:
---

> [!Cite]
> Li, P., Castelo, N., Katona, Z., & Sarvary, M. (2023). _Determining the Validity of Large Language Models for Automated Perceptual Analysis_ (SSRN Scholarly Paper 4241291). [https://doi.org/10.2139/ssrn.4241291](https://doi.org/10.2139/ssrn.4241291)

> [!Abstract]
> > This paper explores the potential of Large Language Models (LLMs) to substitute for human participants in market research. Such LLMs can be used to generate text given a prompt. We argue that perceptual analysis is a particularly promising use case for such automated market research for certain product categories.  The proposed new method generates outputs that closely match those generated from human surveys: agreement rates between human- and LLM- generated data sets reach over 75%. Moreover, this applies for perceptual analysis based on both brand similarity measures and product attribute ratings. The paper demonstrates that for some categories, this new method of fully or partially automated market research will increase the efficiency of market research by meaningfully speeding up the process and potentially reducing the cost. Further results also suggest that with an ever larger training corpus applied to large language models, LLM-based market research will be applicable to answer more nuanced questions based on demographic variables or contextual variation that would be prohibitively expensive or infeasible with human respondents
> 

>[!Synth]
> 
>**Contribution**::
>**Related**:: 

>[!metadata]
> **FirstAuthor**::Peiyao Li
> **Author**::Noah Castelo
> **Author**::Zsolt Katona
> **Author**::Miklos Sarvary
> **Title**:: Determining the Validity of Large Language Models for Automated Perceptual Analysis
> **Year**:: 2023
> **Citekey**:: liDeterminingValidityLarge2023
> **ItemType**:: preprint
> **Location**:: Rochester, NY 
> **DOI**:: 10.2139/ssrn.4241291 

> [!LINK]
> > [Li et al. - 2023 - Determining the Validity of Large Language Models .pdf](file:///Users/brunoamaral/Zotero/storage/3VWUPKTC/Li%20et%20al.%20-%202023%20-%20Determining%20the%20Validity%20of%20Large%20Language%20Models%20.pdf)


# Notes
 [Determining the Validity of Large Language Models for Automated Perceptual Analysis](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4241291)

- **Overview**: Investigating Large Language Models (LLMs) like GPT-Neo 2.7B and ChatGPT (GPT-4) as alternatives to human participants in market research, this paper highlights their high mimicry potential.
- **Key Findings**: Demonstrates over 75% agreement between LLM-generated and human data, suggesting LLMs as effective for market research with a noted 25% potential quality reduction compared to traditional methods.
- **Discussion Highlights**:
  - Adaptability of this method to various generative language models.
  - The necessity of addressing Anglo-Saxon bias in LLM training sets for broader cultural applicability.
  - Reflections on the narrow demographics typically studied and the implications of demographic-focused market research.
- **Concerns & Further Study**: Emphasizes the need to explore LLM effectiveness across cultures and the importance of transparency regarding their training datasets.
- **Conclusion**: LLMs show promise in market research data collection but face challenges regarding cultural bias, data quality, and ethical considerations.



# Annotations

%% begin annotations %%
%% end annotations %%

%% Import Date: 2024-03-04T13:31:34.398+00:00 %%
